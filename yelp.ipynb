{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "from pipe import transform_text_func\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "import logging\n",
    "from gensim import corpora,models,similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------read data-------\n",
      "total sample number:  5261669\n",
      "time elapsed:  225.13795733451843\n"
     ]
    }
   ],
   "source": [
    "print('Read Data......')\n",
    "s=time.time()\n",
    "with open(\"review.json\", encoding=\"utf8\") as f:\n",
    "    reviews = f.read().strip().split(\"\\n\")\n",
    "reviews = [json.loads(review) for review in reviews]\n",
    "x = [reviews[i]['text'] for i in range(len(reviews))]\n",
    "y = [reviews[i]['stars'] for i in range(len(reviews))]\n",
    "del reviews\n",
    "y = [i-1 for i in y]\n",
    "print('total sample number: ',len(x))\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100000\n",
    "x_total = x[0:n_sample]\n",
    "y_total = y[0:n_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_prob(data):\n",
    "    uppercase = []\n",
    "    total = len(data)\n",
    "    step = 0\n",
    "    for i in data:\n",
    "        length = len(i.split())\n",
    "        tmp = []\n",
    "        for j in i:\n",
    "            if j.isupper():\n",
    "                tmp.append(j)\n",
    "        uppercase.append(len(tmp)/length)\n",
    "        bar.drawProgressBar(step/total)\n",
    "        step +=1\n",
    "    return(uppercase)\n",
    "\n",
    "def scale(data):\n",
    "    data = np.array(data).reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    return(data)\n",
    "\n",
    "def tokenize_stop(text):\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.replace(',',' ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "\n",
    "def stop_and_max_feature(data, top_frequent_num, word_least_frequency_num,scale_v,stop_var):\n",
    "    x_lower = [sublist.lower() for sublist in data]\n",
    "    x_lower = [tokenize_stop(i) for i in x_lower]\n",
    "    x_unlist = []\n",
    "    for i in x_lower:\n",
    "        x_unlist += i\n",
    "    vocab_dic = Counter(x_unlist)\n",
    "    stopwords_num = top_frequent_num\n",
    "\n",
    "    print('    total vocab: ',len(vocab_dic.most_common()))\n",
    "    maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>word_least_frequency_num])\n",
    "    print('    vocab size frequency >', word_least_frequency_num, ': ', maxfeature)\n",
    "\n",
    "    stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "\n",
    "    x_n_level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "    x_n_level_unlist = [[] for i in np.unique(y_total)]\n",
    "    for i in range(len(x_n_level)):\n",
    "        for j in x_n_level[i]:\n",
    "            x_n_level_unlist[i] += j\n",
    "        \n",
    "    multilevel_vocab = []\n",
    "    for i in range(np.unique(y_total)):\n",
    "        multilevel_vocab.append(Counter(x_n_level_unlist[i]))\n",
    "\n",
    "    multilevel_stop = defaultdict(list)\n",
    "    for i in range(np.unique(y_total)):\n",
    "        tt = len(x_n_level_unlist[i])\n",
    "        for j in stop:\n",
    "            multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "    stop_var = [(key,np.std(value)*scale_v) for key,value in multilevel_stop.items() ]\n",
    "    stop = [i[0] for i in stop_var if i[1]<stop_var]\n",
    "    return(stop)\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('!','').replace('?','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose specific stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------choose specific stop words-------\n",
      "['the', 'and', 'i', 'a', 'to', 'was', 'of', 'it', 'is', 'for', 'in', 'my', 'that', 'with', 'this', 'they', 'but', 'we', 'you', 'on', 'have', 'not', 'had', 'so', 'were', 'at', 'are', 'food', 'good', 'place', 'be', 'as', 'very', 'me', 'great', 'there', 'all', 'if', 'out', 'like', 'just', 'here', 'service', 'our', 'time', 'get', 'one', 'their', 'from', 'when', 'would', 'or', 'up', \"it's\", 'back', 'an', 'go', 'about', 'will', 'really', 'he', 'what', 'which', 'she', 'been', 'no', 'your', 'some', 'also', 'only', 'can', 'more', 'them', 'us', 'by', 'because', 'other', 'nice', 'got', \"don't\", 'even', 'do', 'after', 'well', \"i'm\", 'has', \"i've\", 'best', 'always', 'too', 'ordered', 'love', 'did', 'than', 'came', 'staff', \"didn't\", 'first', 'order', 'little', 'never', 'went', 'friendly', 'try', 'definitely', 'restaurant', 'much', 'come', 'people', 'could', 'chicken', 'her', 'over', 'then', 'pretty', 'made', 'make', 'again', '2', 'how', 'experience', '5', 'menu', 'going', 'who', 'off', 'know', 'better', 'said', 'two', 'way', 'any', 'day', 'new', 'before', '3', 'wait', 'want', 'am', 'delicious', 'recommend', 'take', 'think', 'right', 'still', 'say', 'amazing', 'see', 'everything', 'around', 'bar', 'sure', 'now', 'few', 'down', 'night', 'ever', 'while', 'give', 'price', 'next', 'fresh', 'took', 'sauce', 'every', 'most', 'his', 'told', 'since', 'eat', 'minutes', 'location', 'work', 'pizza', 'room', 'times', 'bad', 'area', \"wasn't\", 'another', 'bit', 'cheese', 'small', 'being', 'table', 'find', 'asked', 'lot', 'many', 'vegas', \"can't\", 'something', 'lunch', 'though', 'long', '4', 'both', 'into', 'customer', 'side', 'last', 'salad', 'tried', 'where', 'looking', 'worth', 'meal', 'home', 'drinks', 'happy', 'super', 'wanted', 'clean', 'need', 'feel', 'thing', '10', 'dinner', '1', 'big', \"you're\", 'check', 'prices', 'quality', 'hot', 'different', 'enough', 'store', 'should', 'same', 'found', 'nothing', 'car', 'excellent', 'burger', 'taste', 'awesome', 'drink', 'years', 'its', 'server', 'things', 'these', 'fries', 'stars', 'him', 'done', 'away', 'hour', \"that's\"] \n",
      "\n",
      "\n",
      "total vocab:  115414\n",
      "vocab size frequency > 1:  58843\n",
      "time elapsed:  4.082228183746338\n"
     ]
    }
   ],
   "source": [
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "punctuation = string.punctuation.replace('!','').replace('?','').replace(\"'\",'').replace('~','')\n",
    "regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "    text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>1])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose stop words via variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y)==i))) for i in np.unique(y)]\n",
    "x_5level_unlist = [[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(5):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(5):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.1]\n",
    "# import pickle\n",
    "# pickle.dump(multilevel_stop,open('multilevel_stop','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------transform to features-------\n",
      "time elapsed:  210.51831817626953\n"
     ]
    }
   ],
   "source": [
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "\n",
    "\n",
    "\n",
    "review_pipeline = transform_text_func(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                      binary = True, stopwords=stop,token=tokenize,analyzer ='word')\n",
    "X = review_pipeline.fit_transform(x_total)\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add length as new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------add length to features-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kailu2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (100000, 58844)\n",
      "time elapsed:  16.90064287185669\n"
     ]
    }
   ],
   "source": [
    "print('-------add length to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l])\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('-------mutual information feature selection-------')\n",
    "# s = time.time()\n",
    "# from sklearn.feature_selection import f_regression,mutual_info_classif\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# print('Original data size: ',X.shape)\n",
    "# X_new = SelectKBest(mutual_info_classif, k=30000).fit_transform(X, y_total)\n",
    "# print('new data size: ',X_new.shape)\n",
    "# print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:52:58,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-01 23:52:59,200 : INFO : adding document #10000 to Dictionary(38435 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:00,034 : INFO : adding document #20000 to Dictionary(53218 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:00,772 : INFO : adding document #30000 to Dictionary(65114 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:01,507 : INFO : adding document #40000 to Dictionary(74152 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:02,258 : INFO : adding document #50000 to Dictionary(82136 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:02,997 : INFO : adding document #60000 to Dictionary(89479 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:03,715 : INFO : adding document #70000 to Dictionary(96385 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:04,434 : INFO : adding document #80000 to Dictionary(102657 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:05,155 : INFO : adding document #90000 to Dictionary(109638 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...)\n",
      "2018-03-01 23:53:05,896 : INFO : built Dictionary(115414 unique tokens: ['a', 'and', 'around', 'ask', \"can't\"]...) from 100000 documents (total 11333503 corpus positions)\n",
      "2018-03-01 23:53:13,802 : INFO : using symmetric alpha at 0.01\n",
      "2018-03-01 23:53:13,802 : INFO : using symmetric eta at 0.01\n",
      "2018-03-01 23:53:13,816 : INFO : using serial LDA version on this node\n",
      "2018-03-01 23:53:47,837 : INFO : running online (single-pass) LDA training, 100 topics, 1 passes over the supplied corpus of 100000 documents, updating model once every 2000 documents, evaluating perplexity every 20000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-03-01 23:53:47,837 : INFO : PROGRESS: pass 0, at document #2000/100000\n",
      "2018-03-01 23:53:55,037 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:53:56,633 : INFO : topic #63 (0.010): 0.045*\"the\" + 0.030*\"and\" + 0.017*\"it\" + 0.016*\"i\" + 0.015*\"you\" + 0.014*\"in\" + 0.014*\"with\" + 0.014*\"is\" + 0.014*\"for\" + 0.013*\"to\"\n",
      "2018-03-01 23:53:56,633 : INFO : topic #81 (0.010): 0.021*\"the\" + 0.017*\"was\" + 0.016*\"to\" + 0.014*\"de\" + 0.013*\"i\" + 0.013*\"la\" + 0.012*\"et\" + 0.012*\"and\" + 0.012*\"un\" + 0.012*\"of\"\n",
      "2018-03-01 23:53:56,633 : INFO : topic #2 (0.010): 0.034*\"i\" + 0.033*\"and\" + 0.032*\"to\" + 0.030*\"the\" + 0.021*\"a\" + 0.016*\"of\" + 0.015*\"in\" + 0.013*\"it\" + 0.011*\"on\" + 0.010*\"have\"\n",
      "2018-03-01 23:53:56,649 : INFO : topic #14 (0.010): 0.041*\"the\" + 0.036*\"a\" + 0.035*\"i\" + 0.027*\"to\" + 0.018*\"and\" + 0.017*\"of\" + 0.012*\"is\" + 0.010*\"it\" + 0.010*\"that\" + 0.010*\"in\"\n",
      "2018-03-01 23:53:56,649 : INFO : topic #6 (0.010): 0.028*\"and\" + 0.025*\"a\" + 0.023*\"the\" + 0.016*\"of\" + 0.015*\"to\" + 0.012*\"is\" + 0.012*\"de\" + 0.011*\"i\" + 0.011*\"you\" + 0.009*\"on\"\n",
      "2018-03-01 23:53:56,678 : INFO : topic diff=95.526382, rho=1.000000\n",
      "2018-03-01 23:53:56,693 : INFO : PROGRESS: pass 0, at document #4000/100000\n",
      "C:\\Users\\kailu2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "2018-03-01 23:54:02,717 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:04,317 : INFO : topic #65 (0.010): 0.038*\"a\" + 0.037*\"the\" + 0.034*\"and\" + 0.026*\"was\" + 0.015*\"i\" + 0.015*\"of\" + 0.014*\"to\" + 0.014*\"for\" + 0.013*\"with\" + 0.013*\"it\"\n",
      "2018-03-01 23:54:04,317 : INFO : topic #29 (0.010): 0.059*\"the\" + 0.044*\"was\" + 0.035*\"and\" + 0.028*\"a\" + 0.027*\"i\" + 0.021*\"it\" + 0.014*\"of\" + 0.013*\"to\" + 0.012*\"had\" + 0.012*\"with\"\n",
      "2018-03-01 23:54:04,317 : INFO : topic #5 (0.010): 0.044*\"the\" + 0.039*\"and\" + 0.039*\"i\" + 0.029*\"to\" + 0.025*\"a\" + 0.014*\"of\" + 0.012*\"in\" + 0.010*\"my\" + 0.009*\"for\" + 0.009*\"that\"\n",
      "2018-03-01 23:54:04,325 : INFO : topic #93 (0.010): 0.051*\"the\" + 0.036*\"i\" + 0.034*\"and\" + 0.031*\"to\" + 0.026*\"a\" + 0.018*\"it\" + 0.017*\"of\" + 0.016*\"was\" + 0.016*\"for\" + 0.012*\"with\"\n",
      "2018-03-01 23:54:04,325 : INFO : topic #58 (0.010): 0.036*\"de\" + 0.027*\"le\" + 0.024*\"et\" + 0.019*\"la\" + 0.019*\"un\" + 0.014*\"à\" + 0.013*\"pour\" + 0.012*\"il\" + 0.012*\"je\" + 0.012*\"a\"\n",
      "2018-03-01 23:54:04,357 : INFO : topic diff=inf, rho=0.707107\n",
      "2018-03-01 23:54:04,367 : INFO : PROGRESS: pass 0, at document #6000/100000\n",
      "2018-03-01 23:54:10,603 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:12,082 : INFO : topic #54 (0.010): 0.034*\"and\" + 0.033*\"the\" + 0.023*\"a\" + 0.019*\"i\" + 0.015*\"theatre\" + 0.015*\"to\" + 0.014*\"is\" + 0.010*\"of\" + 0.009*\"that\" + 0.008*\"in\"\n",
      "2018-03-01 23:54:12,098 : INFO : topic #10 (0.010): 0.036*\"i\" + 0.029*\"the\" + 0.023*\"and\" + 0.019*\"to\" + 0.017*\"my\" + 0.015*\"a\" + 0.013*\"that\" + 0.012*\"you\" + 0.012*\"is\" + 0.010*\"they\"\n",
      "2018-03-01 23:54:12,098 : INFO : topic #27 (0.010): 0.044*\"the\" + 0.025*\"a\" + 0.017*\"i\" + 0.017*\"to\" + 0.016*\"and\" + 0.013*\"of\" + 0.012*\"is\" + 0.012*\"it\" + 0.011*\"on\" + 0.010*\"that\"\n",
      "2018-03-01 23:54:12,101 : INFO : topic #80 (0.010): 0.042*\"and\" + 0.040*\"the\" + 0.033*\"to\" + 0.026*\"i\" + 0.021*\"a\" + 0.018*\"is\" + 0.018*\"they\" + 0.015*\"you\" + 0.015*\"of\" + 0.012*\"for\"\n",
      "2018-03-01 23:54:12,101 : INFO : topic #83 (0.010): 0.062*\"the\" + 0.036*\"to\" + 0.031*\"and\" + 0.028*\"a\" + 0.027*\"i\" + 0.023*\"was\" + 0.017*\"of\" + 0.014*\"it\" + 0.013*\"in\" + 0.011*\"but\"\n",
      "2018-03-01 23:54:12,122 : INFO : topic diff=inf, rho=0.577350\n",
      "2018-03-01 23:54:12,143 : INFO : PROGRESS: pass 0, at document #8000/100000\n",
      "2018-03-01 23:54:18,096 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:19,488 : INFO : topic #97 (0.010): 0.031*\"only!\" + 0.020*\"sucre\" + 0.020*\"cabane\" + 0.013*\"heavenly\" + 0.011*\"activités\" + 0.007*\"a\" + 0.006*\"parfaite\" + 0.005*\"y\" + 0.004*\"d'autres\" + 0.004*\"pour\"\n",
      "2018-03-01 23:54:19,488 : INFO : topic #39 (0.010): 0.030*\"the\" + 0.029*\"app\" + 0.026*\"and\" + 0.024*\"best!\" + 0.024*\"amazing\" + 0.019*\"yesterday\" + 0.017*\"fiance\" + 0.017*\"x\" + 0.017*\"married\" + 0.016*\"flavors\"\n",
      "2018-03-01 23:54:19,488 : INFO : topic #22 (0.010): 0.063*\"the\" + 0.037*\"and\" + 0.034*\"a\" + 0.028*\"i\" + 0.024*\"was\" + 0.016*\"to\" + 0.016*\"it\" + 0.016*\"for\" + 0.012*\"in\" + 0.012*\"of\"\n",
      "2018-03-01 23:54:19,493 : INFO : topic #70 (0.010): 0.071*\"sauces\" + 0.055*\"scene\" + 0.029*\"pig\" + 0.026*\"the\" + 0.021*\"for!\" + 0.021*\"everywhere\" + 0.019*\"approval\" + 0.018*\"pretzels\" + 0.016*\"stupid\" + 0.015*\"and\"\n",
      "2018-03-01 23:54:19,493 : INFO : topic #6 (0.010): 0.033*\"dumplings\" + 0.033*\"noodle\" + 0.022*\"and\" + 0.019*\"the\" + 0.018*\"a\" + 0.014*\"lukewarm\" + 0.013*\"of\" + 0.013*\"noodles\" + 0.013*\"approximately\" + 0.012*\"to\"\n",
      "2018-03-01 23:54:19,521 : INFO : topic diff=inf, rho=0.500000\n",
      "2018-03-01 23:54:19,537 : INFO : PROGRESS: pass 0, at document #10000/100000\n",
      "2018-03-01 23:54:25,433 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:26,783 : INFO : topic #56 (0.010): 0.052*\"i\" + 0.040*\"and\" + 0.038*\"the\" + 0.031*\"a\" + 0.024*\"to\" + 0.018*\"my\" + 0.018*\"was\" + 0.017*\"of\" + 0.014*\"in\" + 0.012*\"that\"\n",
      "2018-03-01 23:54:26,799 : INFO : topic #85 (0.010): 0.057*\"the\" + 0.031*\"and\" + 0.021*\"to\" + 0.016*\"is\" + 0.015*\"of\" + 0.013*\"i\" + 0.013*\"a\" + 0.012*\"in\" + 0.012*\"authentic\" + 0.011*\"are\"\n",
      "2018-03-01 23:54:26,799 : INFO : topic #34 (0.010): 0.065*\"we\" + 0.053*\"and\" + 0.046*\"the\" + 0.028*\"was\" + 0.024*\"a\" + 0.024*\"to\" + 0.020*\"were\" + 0.018*\"i\" + 0.016*\"our\" + 0.014*\"but\"\n",
      "2018-03-01 23:54:26,801 : INFO : topic #29 (0.010): 0.066*\"the\" + 0.057*\"was\" + 0.041*\"and\" + 0.030*\"a\" + 0.030*\"i\" + 0.023*\"it\" + 0.015*\"with\" + 0.014*\"of\" + 0.013*\"had\" + 0.013*\"but\"\n",
      "2018-03-01 23:54:26,801 : INFO : topic #22 (0.010): 0.064*\"the\" + 0.038*\"and\" + 0.033*\"a\" + 0.029*\"i\" + 0.023*\"was\" + 0.016*\"to\" + 0.016*\"for\" + 0.015*\"it\" + 0.012*\"in\" + 0.012*\"of\"\n",
      "2018-03-01 23:54:26,831 : INFO : topic diff=inf, rho=0.447214\n",
      "2018-03-01 23:54:26,831 : INFO : PROGRESS: pass 0, at document #12000/100000\n",
      "2018-03-01 23:54:32,856 : INFO : merging changes from 2000 documents into a model of 100000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:54:34,182 : INFO : topic #13 (0.010): 0.037*\"and\" + 0.034*\"buffet\" + 0.029*\"the\" + 0.025*\"average\" + 0.024*\"for\" + 0.023*\"was\" + 0.023*\"food\" + 0.021*\"priced\" + 0.020*\"pasta\" + 0.019*\"a\"\n",
      "2018-03-01 23:54:34,182 : INFO : topic #5 (0.010): 0.041*\"the\" + 0.040*\"and\" + 0.039*\"to\" + 0.038*\"i\" + 0.021*\"a\" + 0.015*\"he\" + 0.015*\"my\" + 0.012*\"they\" + 0.012*\"in\" + 0.011*\"of\"\n",
      "2018-03-01 23:54:34,182 : INFO : topic #41 (0.010): 0.076*\"hamburger\" + 0.059*\"!!!\" + 0.055*\"chimi\" + 0.054*\"ya\" + 0.051*\"monte\" + 0.042*\"frites\" + 0.012*\"la\" + 0.012*\"de\" + 0.012*\"trop\" + 0.011*\"un\"\n",
      "2018-03-01 23:54:34,197 : INFO : topic #88 (0.010): 0.050*\"and\" + 0.042*\"the\" + 0.035*\"a\" + 0.031*\"to\" + 0.029*\"is\" + 0.025*\"great\" + 0.025*\"you\" + 0.019*\"place\" + 0.017*\"are\" + 0.016*\"i\"\n",
      "2018-03-01 23:54:34,199 : INFO : topic #28 (0.010): 0.032*\"a\" + 0.031*\"the\" + 0.030*\"and\" + 0.022*\"is\" + 0.018*\"of\" + 0.018*\"i\" + 0.017*\"to\" + 0.014*\"for\" + 0.012*\"in\" + 0.012*\"it\"\n",
      "2018-03-01 23:54:34,228 : INFO : topic diff=inf, rho=0.408248\n",
      "2018-03-01 23:54:34,228 : INFO : PROGRESS: pass 0, at document #14000/100000\n",
      "2018-03-01 23:54:41,086 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:42,353 : INFO : topic #10 (0.010): 0.037*\"wash\" + 0.029*\"car\" + 0.028*\"i\" + 0.025*\"the\" + 0.020*\"computer\" + 0.019*\"con\" + 0.019*\"and\" + 0.018*\"clinic\" + 0.016*\"to\" + 0.015*\"my\"\n",
      "2018-03-01 23:54:42,353 : INFO : topic #96 (0.010): 0.077*\"auto\" + 0.074*\"donuts\" + 0.044*\"tile\" + 0.039*\"donut\" + 0.026*\"the\" + 0.023*\"mornings\" + 0.019*\"co\" + 0.017*\"quesadilla\" + 0.017*\"queen\" + 0.017*\"ah\"\n",
      "2018-03-01 23:54:42,353 : INFO : topic #45 (0.010): 0.166*\"!!\" + 0.067*\"stephanie\" + 0.034*\"beth\" + 0.017*\"zumba\" + 0.016*\"coupe\" + 0.013*\"lasagne\" + 0.009*\"de\" + 0.007*\"bon\" + 0.007*\"et\" + 0.006*\"à\"\n",
      "2018-03-01 23:54:42,362 : INFO : topic #8 (0.010): 0.034*\"im\" + 0.032*\"worst\" + 0.022*\"pot\" + 0.021*\"loves\" + 0.021*\"gonna\" + 0.020*\"feed\" + 0.019*\"furniture\" + 0.018*\"chris\" + 0.017*\"hot\" + 0.016*\"waste\"\n",
      "2018-03-01 23:54:42,362 : INFO : topic #40 (0.010): 0.088*\"risotto\" + 0.063*\"cirque\" + 0.059*\"montreal\" + 0.050*\"favourite\" + 0.039*\"ho\" + 0.038*\"du\" + 0.037*\"wave\" + 0.036*\"chinatown\" + 0.032*\"remind\" + 0.031*\"lo\"\n",
      "2018-03-01 23:54:42,393 : INFO : topic diff=inf, rho=0.377964\n",
      "2018-03-01 23:54:42,393 : INFO : PROGRESS: pass 0, at document #16000/100000\n",
      "2018-03-01 23:54:48,770 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:50,037 : INFO : topic #31 (0.010): 0.153*\"toast\" + 0.128*\"french\" + 0.058*\"soooo\" + 0.055*\"shirts\" + 0.049*\"beats\" + 0.035*\"closet\" + 0.031*\"satisfy\" + 0.015*\"'s\" + 0.015*\"fails\" + 0.014*\"denny's\"\n",
      "2018-03-01 23:54:50,037 : INFO : topic #21 (0.010): 0.043*\"best\" + 0.041*\"ever\" + 0.036*\"and\" + 0.031*\"baked\" + 0.031*\"the\" + 0.028*\"coffee\" + 0.023*\"delivery\" + 0.023*\"house\" + 0.023*\"so\" + 0.022*\"i\"\n",
      "2018-03-01 23:54:50,053 : INFO : topic #24 (0.010): 0.057*\"the\" + 0.033*\"and\" + 0.025*\"a\" + 0.020*\"but\" + 0.020*\"i\" + 0.019*\"is\" + 0.017*\"it's\" + 0.015*\"it\" + 0.013*\"of\" + 0.013*\"good\"\n",
      "2018-03-01 23:54:50,054 : INFO : topic #46 (0.010): 0.066*\"and\" + 0.054*\"the\" + 0.049*\"is\" + 0.040*\"very\" + 0.035*\"always\" + 0.031*\"service\" + 0.027*\"staff\" + 0.025*\"friendly\" + 0.021*\"great\" + 0.020*\"to\"\n",
      "2018-03-01 23:54:50,054 : INFO : topic #45 (0.010): 0.181*\"!!\" + 0.058*\"stephanie\" + 0.026*\"beth\" + 0.013*\"zumba\" + 0.012*\"coupe\" + 0.010*\"lasagne\" + 0.008*\"russe\" + 0.007*\"workout!\" + 0.007*\"de\" + 0.006*\"bon\"\n",
      "2018-03-01 23:54:50,087 : INFO : topic diff=inf, rho=0.353553\n",
      "2018-03-01 23:54:50,087 : INFO : PROGRESS: pass 0, at document #18000/100000\n",
      "2018-03-01 23:54:56,389 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:54:57,648 : INFO : topic #88 (0.010): 0.048*\"and\" + 0.044*\"the\" + 0.039*\"a\" + 0.031*\"to\" + 0.031*\"is\" + 0.024*\"you\" + 0.022*\"great\" + 0.018*\"place\" + 0.016*\"for\" + 0.016*\"of\"\n",
      "2018-03-01 23:54:57,648 : INFO : topic #93 (0.010): 0.052*\"the\" + 0.035*\"i\" + 0.033*\"and\" + 0.032*\"to\" + 0.027*\"a\" + 0.019*\"of\" + 0.016*\"it\" + 0.015*\"for\" + 0.014*\"with\" + 0.014*\"was\"\n",
      "2018-03-01 23:54:57,648 : INFO : topic #89 (0.010): 0.054*\"the\" + 0.048*\"wings\" + 0.032*\"and\" + 0.025*\"a\" + 0.021*\"is\" + 0.017*\"groupon\" + 0.017*\"back!\" + 0.015*\"great\" + 0.014*\"consistent\" + 0.013*\"of\"\n",
      "2018-03-01 23:54:57,658 : INFO : topic #44 (0.010): 0.050*\"i\" + 0.036*\"the\" + 0.036*\"a\" + 0.033*\"and\" + 0.030*\"was\" + 0.026*\"to\" + 0.015*\"that\" + 0.014*\"in\" + 0.012*\"my\" + 0.012*\"it\"\n",
      "2018-03-01 23:54:57,658 : INFO : topic #5 (0.010): 0.043*\"the\" + 0.041*\"to\" + 0.040*\"and\" + 0.035*\"i\" + 0.021*\"a\" + 0.015*\"my\" + 0.014*\"they\" + 0.014*\"he\" + 0.012*\"in\" + 0.011*\"for\"\n",
      "2018-03-01 23:54:57,679 : INFO : topic diff=inf, rho=0.333333\n",
      "2018-03-01 23:55:06,547 : INFO : -11.508 per-word bound, 2912.6 perplexity estimate based on a held-out corpus of 2000 documents with 221868 words\n",
      "2018-03-01 23:55:06,547 : INFO : PROGRESS: pass 0, at document #20000/100000\n",
      "2018-03-01 23:55:12,499 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:13,796 : INFO : topic #62 (0.010): 0.057*\"the\" + 0.048*\"and\" + 0.034*\"was\" + 0.032*\"soup\" + 0.029*\"noodles\" + 0.028*\"ramen\" + 0.025*\"curry\" + 0.025*\"chicken\" + 0.024*\"rice\" + 0.023*\"i\"\n",
      "2018-03-01 23:55:13,796 : INFO : topic #23 (0.010): 0.047*\"to\" + 0.045*\"the\" + 0.035*\"a\" + 0.025*\"and\" + 0.017*\"i\" + 0.014*\"for\" + 0.013*\"in\" + 0.013*\"it\" + 0.013*\"of\" + 0.012*\"you\"\n",
      "2018-03-01 23:55:13,796 : INFO : topic #80 (0.010): 0.041*\"you\" + 0.038*\"to\" + 0.036*\"the\" + 0.031*\"and\" + 0.025*\"they\" + 0.021*\"i\" + 0.020*\"is\" + 0.019*\"a\" + 0.016*\"if\" + 0.015*\"have\"\n",
      "2018-03-01 23:55:13,810 : INFO : topic #60 (0.010): 0.086*\"the\" + 0.039*\"is\" + 0.031*\"a\" + 0.031*\"and\" + 0.024*\"in\" + 0.023*\"to\" + 0.023*\"you\" + 0.022*\"of\" + 0.018*\"are\" + 0.016*\"vegas\"\n",
      "2018-03-01 23:55:13,810 : INFO : topic #72 (0.010): 0.050*\"wow!\" + 0.047*\"2013\" + 0.026*\"royal\" + 0.025*\"doughy\" + 0.022*\"i\" + 0.016*\"respond\" + 0.016*\"the\" + 0.016*\"embarrassing\" + 0.015*\"treatments\" + 0.014*\"to\"\n",
      "2018-03-01 23:55:13,842 : INFO : topic diff=inf, rho=0.316228\n",
      "2018-03-01 23:55:13,842 : INFO : PROGRESS: pass 0, at document #22000/100000\n",
      "2018-03-01 23:55:20,022 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:21,263 : INFO : topic #18 (0.010): 0.057*\"the\" + 0.045*\"i\" + 0.043*\"was\" + 0.031*\"to\" + 0.031*\"it\" + 0.029*\"and\" + 0.015*\"a\" + 0.015*\"food\" + 0.013*\"but\" + 0.012*\"my\"\n",
      "2018-03-01 23:55:21,263 : INFO : topic #83 (0.010): 0.068*\"the\" + 0.036*\"to\" + 0.029*\"a\" + 0.028*\"and\" + 0.024*\"i\" + 0.023*\"was\" + 0.020*\"of\" + 0.015*\"it\" + 0.014*\"in\" + 0.011*\"that\"\n",
      "2018-03-01 23:55:21,263 : INFO : topic #37 (0.010): 0.069*\"the\" + 0.051*\"is\" + 0.029*\"a\" + 0.020*\"good\" + 0.019*\"and\" + 0.018*\"but\" + 0.017*\"are\" + 0.017*\"you\" + 0.016*\"to\" + 0.016*\"food\"\n",
      "2018-03-01 23:55:21,275 : INFO : topic #56 (0.010): 0.056*\"i\" + 0.039*\"and\" + 0.039*\"the\" + 0.032*\"a\" + 0.025*\"to\" + 0.023*\"my\" + 0.018*\"of\" + 0.017*\"was\" + 0.014*\"in\" + 0.012*\"it\"\n",
      "2018-03-01 23:55:21,275 : INFO : topic #70 (0.010): 0.142*\"sauces\" + 0.070*\"scene\" + 0.054*\"everywhere\" + 0.043*\"stupid\" + 0.042*\"for!\" + 0.040*\"concept\" + 0.036*\"puts\" + 0.030*\"pig\" + 0.022*\"pretzels\" + 0.022*\"whoever\"\n",
      "2018-03-01 23:55:21,307 : INFO : topic diff=inf, rho=0.301511\n",
      "2018-03-01 23:55:21,323 : INFO : PROGRESS: pass 0, at document #24000/100000\n",
      "2018-03-01 23:55:27,364 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:28,604 : INFO : topic #75 (0.010): 0.200*\"mins\" + 0.157*\"24\" + 0.067*\"30pm\" + 0.059*\"subway\" + 0.049*\"tooth\" + 0.047*\"hours\" + 0.036*\"staffed\" + 0.030*\"open\" + 0.027*\"ruin\" + 0.018*\"doughnuts\"\n",
      "2018-03-01 23:55:28,604 : INFO : topic #85 (0.010): 0.047*\"the\" + 0.024*\"and\" + 0.021*\"mom\" + 0.020*\"authentic\" + 0.017*\"fish\" + 0.016*\"falafel\" + 0.016*\"to\" + 0.014*\"girls\" + 0.013*\"is\" + 0.012*\"of\"\n",
      "2018-03-01 23:55:28,604 : INFO : topic #37 (0.010): 0.069*\"the\" + 0.052*\"is\" + 0.029*\"a\" + 0.021*\"good\" + 0.019*\"and\" + 0.018*\"but\" + 0.018*\"food\" + 0.017*\"are\" + 0.016*\"to\" + 0.016*\"you\"\n",
      "2018-03-01 23:55:28,615 : INFO : topic #9 (0.010): 0.050*\"the\" + 0.038*\"a\" + 0.036*\"and\" + 0.017*\"chocolate\" + 0.016*\"in\" + 0.016*\"to\" + 0.016*\"der\" + 0.015*\"is\" + 0.013*\"of\" + 0.013*\"for\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:55:28,615 : INFO : topic #18 (0.010): 0.058*\"the\" + 0.045*\"i\" + 0.044*\"was\" + 0.031*\"to\" + 0.030*\"it\" + 0.030*\"and\" + 0.016*\"food\" + 0.015*\"a\" + 0.013*\"not\" + 0.012*\"but\"\n",
      "2018-03-01 23:55:28,634 : INFO : topic diff=inf, rho=0.288675\n",
      "2018-03-01 23:55:28,650 : INFO : PROGRESS: pass 0, at document #26000/100000\n",
      "2018-03-01 23:55:34,656 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:35,895 : INFO : topic #71 (0.010): 0.229*\"sushi\" + 0.086*\"drive\" + 0.045*\"thru\" + 0.038*\"chef\" + 0.019*\"service\" + 0.019*\"is\" + 0.019*\"the\" + 0.018*\"employees\" + 0.017*\"friendly!\" + 0.016*\"items\"\n",
      "2018-03-01 23:55:35,895 : INFO : topic #68 (0.010): 0.122*\"station\" + 0.055*\"gas\" + 0.055*\"shake\" + 0.040*\"pastries\" + 0.032*\"smoothies\" + 0.029*\"chow\" + 0.027*\"mein\" + 0.023*\"wasting\" + 0.023*\"l\" + 0.022*\"convenience\"\n",
      "2018-03-01 23:55:35,899 : INFO : topic #29 (0.010): 0.075*\"the\" + 0.074*\"was\" + 0.047*\"and\" + 0.034*\"i\" + 0.032*\"a\" + 0.027*\"it\" + 0.020*\"had\" + 0.015*\"with\" + 0.014*\"of\" + 0.013*\"very\"\n",
      "2018-03-01 23:55:35,899 : INFO : topic #82 (0.010): 0.069*\"tip\" + 0.066*\"vehicle\" + 0.048*\"central\" + 0.047*\"slice\" + 0.037*\"driving\" + 0.037*\"charlotte\" + 0.026*\"bigger\" + 0.025*\"range\" + 0.024*\"thrilled\" + 0.020*\"bad\"\n",
      "2018-03-01 23:55:35,899 : INFO : topic #12 (0.010): 0.142*\"pizza\" + 0.042*\"the\" + 0.038*\"and\" + 0.035*\"a\" + 0.026*\"crust\" + 0.021*\"cheese\" + 0.018*\"lobster\" + 0.018*\"of\" + 0.017*\"it\" + 0.017*\"sausage\"\n",
      "2018-03-01 23:55:35,932 : INFO : topic diff=inf, rho=0.277350\n",
      "2018-03-01 23:55:35,932 : INFO : PROGRESS: pass 0, at document #28000/100000\n",
      "2018-03-01 23:55:42,142 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:43,380 : INFO : topic #62 (0.010): 0.059*\"5\" + 0.049*\"the\" + 0.042*\"and\" + 0.040*\"soup\" + 0.027*\"chicken\" + 0.027*\"thai\" + 0.026*\"rice\" + 0.026*\"curry\" + 0.026*\"was\" + 0.022*\"noodles\"\n",
      "2018-03-01 23:55:43,380 : INFO : topic #11 (0.010): 0.425*\"club\" + 0.068*\"grand\" + 0.041*\"comedy\" + 0.035*\"switched\" + 0.027*\"ears\" + 0.026*\"hangover\" + 0.023*\"wipe\" + 0.021*\"manger\" + 0.018*\"dye\" + 0.009*\"dis\"\n",
      "2018-03-01 23:55:43,394 : INFO : topic #93 (0.010): 0.049*\"the\" + 0.032*\"i\" + 0.031*\"and\" + 0.030*\"to\" + 0.027*\"a\" + 0.020*\"of\" + 0.014*\"it\" + 0.014*\"with\" + 0.014*\"for\" + 0.012*\"was\"\n",
      "2018-03-01 23:55:43,394 : INFO : topic #84 (0.010): 0.052*\"art\" + 0.041*\"bartenders\" + 0.036*\"sub\" + 0.035*\"basic\" + 0.033*\"list\" + 0.029*\"law\" + 0.029*\"stuff\" + 0.024*\"par\" + 0.022*\"canadian\" + 0.015*\"spoiled\"\n",
      "2018-03-01 23:55:43,394 : INFO : topic #72 (0.010): 0.064*\"wow!\" + 0.043*\"respond\" + 0.036*\"2013\" + 0.029*\"royal\" + 0.029*\"cleanliness\" + 0.027*\"learn\" + 0.023*\"sore\" + 0.022*\"importantly\" + 0.021*\"ipa\" + 0.019*\"doughy\"\n",
      "2018-03-01 23:55:43,428 : INFO : topic diff=inf, rho=0.267261\n",
      "2018-03-01 23:55:43,428 : INFO : PROGRESS: pass 0, at document #30000/100000\n",
      "2018-03-01 23:55:49,922 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:51,149 : INFO : topic #7 (0.010): 0.106*\"tacos\" + 0.081*\"mexican\" + 0.058*\"chips\" + 0.044*\"beans\" + 0.024*\"the\" + 0.020*\"tortilla\" + 0.018*\"spot\" + 0.017*\"food\" + 0.017*\"el\" + 0.016*\"tortillas\"\n",
      "2018-03-01 23:55:51,149 : INFO : topic #87 (0.010): 0.027*\"a\" + 0.024*\"the\" + 0.020*\"wing\" + 0.019*\"and\" + 0.019*\"i\" + 0.016*\"kim\" + 0.015*\"of\" + 0.013*\"coke\" + 0.011*\"for\" + 0.011*\"diet\"\n",
      "2018-03-01 23:55:51,149 : INFO : topic #27 (0.010): 0.055*\"exceptional\" + 0.054*\"paint\" + 0.037*\"custom\" + 0.033*\"pretzel\" + 0.028*\"chick\" + 0.022*\"cheapest\" + 0.021*\"project\" + 0.021*\"created\" + 0.019*\"agent\" + 0.016*\"theres\"\n",
      "2018-03-01 23:55:51,155 : INFO : topic #59 (0.010): 0.119*\"closed\" + 0.054*\"tons\" + 0.053*\"businesses\" + 0.040*\"chop\" + 0.038*\"changes\" + 0.037*\"hawaiian\" + 0.033*\"enchilada\" + 0.023*\"tools\" + 0.022*\"shift\" + 0.021*\"5pm\"\n",
      "2018-03-01 23:55:51,155 : INFO : topic #24 (0.010): 0.057*\"the\" + 0.033*\"and\" + 0.023*\"a\" + 0.017*\"is\" + 0.017*\"i\" + 0.016*\"but\" + 0.015*\"sweet\" + 0.013*\"flavors\" + 0.013*\"gelato\" + 0.012*\"of\"\n",
      "2018-03-01 23:55:51,176 : INFO : topic diff=inf, rho=0.258199\n",
      "2018-03-01 23:55:51,192 : INFO : PROGRESS: pass 0, at document #32000/100000\n",
      "2018-03-01 23:55:57,783 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:55:59,008 : INFO : topic #52 (0.010): 0.048*\"the\" + 0.031*\"and\" + 0.027*\"a\" + 0.026*\"of\" + 0.019*\"in\" + 0.019*\"for\" + 0.016*\"to\" + 0.016*\"is\" + 0.015*\"you\" + 0.011*\"are\"\n",
      "2018-03-01 23:55:59,008 : INFO : topic #13 (0.010): 0.061*\"buffet\" + 0.043*\"priced\" + 0.038*\"food\" + 0.034*\"and\" + 0.033*\"indian\" + 0.032*\"pasta\" + 0.028*\"for\" + 0.025*\"average\" + 0.024*\"wedding\" + 0.023*\"the\"\n",
      "2018-03-01 23:55:59,008 : INFO : topic #77 (0.010): 0.049*\"i\" + 0.041*\"a\" + 0.038*\"and\" + 0.037*\"to\" + 0.034*\"was\" + 0.029*\"my\" + 0.026*\"the\" + 0.019*\"very\" + 0.018*\"for\" + 0.014*\"have\"\n",
      "2018-03-01 23:55:59,023 : INFO : topic #17 (0.010): 0.154*\"dog\" + 0.134*\"hot\" + 0.099*\"dogs\" + 0.044*\"chicago\" + 0.029*\"frozen\" + 0.019*\"disappointed!\" + 0.018*\"costco\" + 0.016*\"ordinary\" + 0.016*\"crew\" + 0.015*\"spot!\"\n",
      "2018-03-01 23:55:59,023 : INFO : topic #99 (0.010): 0.099*\"address\" + 0.096*\"maintenance\" + 0.071*\"styles\" + 0.066*\"carte\" + 0.064*\"monthly\" + 0.043*\"1000\" + 0.034*\"la\" + 0.029*\"personnel\" + 0.027*\"rancheros\" + 0.025*\"huevos\"\n",
      "2018-03-01 23:55:59,057 : INFO : topic diff=inf, rho=0.250000\n",
      "2018-03-01 23:55:59,057 : INFO : PROGRESS: pass 0, at document #34000/100000\n",
      "2018-03-01 23:56:05,660 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:56:06,874 : INFO : topic #73 (0.010): 0.058*\"class\" + 0.048*\"2\" + 0.038*\"lol\" + 0.036*\"here!\" + 0.032*\"1\" + 0.025*\"3\" + 0.021*\"classes\" + 0.016*\"deli\" + 0.015*\"a\" + 0.014*\"anywhere\"\n",
      "2018-03-01 23:56:06,874 : INFO : topic #45 (0.010): 0.260*\"brunch\" + 0.142*\"!!\" + 0.046*\"stephanie\" + 0.017*\"instructors\" + 0.015*\"beth\" + 0.011*\"zumba\" + 0.005*\"lasagne\" + 0.005*\"proprio\" + 0.004*\"coupe\" + 0.003*\"pro!\"\n",
      "2018-03-01 23:56:06,874 : INFO : topic #50 (0.010): 0.112*\"repeat\" + 0.058*\"rush\" + 0.058*\"pros\" + 0.053*\"penny\" + 0.052*\"cons\" + 0.048*\"animal\" + 0.047*\"style\" + 0.037*\"premium\" + 0.035*\"bones\" + 0.033*\"boss\"\n",
      "2018-03-01 23:56:06,883 : INFO : topic #44 (0.010): 0.045*\"i\" + 0.035*\"the\" + 0.033*\"a\" + 0.030*\"was\" + 0.029*\"and\" + 0.024*\"to\" + 0.015*\"in\" + 0.014*\"that\" + 0.012*\"of\" + 0.012*\"my\"\n",
      "2018-03-01 23:56:06,883 : INFO : topic #11 (0.010): 0.381*\"club\" + 0.113*\"grand\" + 0.038*\"comedy\" + 0.036*\"switched\" + 0.027*\"hangover\" + 0.023*\"dye\" + 0.022*\"manger\" + 0.021*\"wipe\" + 0.020*\"angela\" + 0.018*\"ears\"\n",
      "2018-03-01 23:56:06,911 : INFO : topic diff=inf, rho=0.242536\n",
      "2018-03-01 23:56:06,911 : INFO : PROGRESS: pass 0, at document #36000/100000\n",
      "2018-03-01 23:56:13,153 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:56:14,436 : INFO : topic #57 (0.010): 0.109*\"country\" + 0.097*\"turkey\" + 0.073*\"boxes\" + 0.048*\"jack\" + 0.039*\"wonderful!\" + 0.034*\"competitive\" + 0.033*\"alfredo\" + 0.023*\"everything!\" + 0.023*\"adjustment\" + 0.022*\"peel\"\n",
      "2018-03-01 23:56:14,436 : INFO : topic #25 (0.010): 0.108*\"knowledgable\" + 0.099*\"repairs\" + 0.096*\"pleasure\" + 0.074*\"he's\" + 0.066*\"lately\" + 0.048*\"branch\" + 0.042*\"freshest\" + 0.035*\"gun\" + 0.028*\"shop!\" + 0.022*\"link\"\n",
      "2018-03-01 23:56:14,436 : INFO : topic #42 (0.010): 0.060*\"the\" + 0.053*\"a\" + 0.038*\"and\" + 0.030*\"of\" + 0.023*\"was\" + 0.023*\"to\" + 0.016*\"for\" + 0.014*\"in\" + 0.012*\"with\" + 0.012*\"were\"\n",
      "2018-03-01 23:56:14,441 : INFO : topic #30 (0.010): 0.039*\"i\" + 0.039*\"was\" + 0.029*\"it\" + 0.029*\"to\" + 0.028*\"the\" + 0.024*\"and\" + 0.023*\"for\" + 0.022*\"they\" + 0.021*\"my\" + 0.021*\"minutes\"\n",
      "2018-03-01 23:56:14,441 : INFO : topic #50 (0.010): 0.100*\"repeat\" + 0.067*\"rush\" + 0.061*\"penny\" + 0.055*\"pros\" + 0.053*\"cons\" + 0.045*\"style\" + 0.043*\"bones\" + 0.042*\"animal\" + 0.039*\"boss\" + 0.033*\"premium\"\n",
      "2018-03-01 23:56:14,467 : INFO : topic diff=inf, rho=0.235702\n",
      "2018-03-01 23:56:14,483 : INFO : PROGRESS: pass 0, at document #38000/100000\n",
      "2018-03-01 23:56:20,962 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:56:22,191 : INFO : topic #52 (0.010): 0.048*\"the\" + 0.030*\"and\" + 0.027*\"a\" + 0.026*\"of\" + 0.019*\"in\" + 0.018*\"for\" + 0.016*\"to\" + 0.015*\"is\" + 0.015*\"you\" + 0.010*\"selection\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:56:22,191 : INFO : topic #56 (0.010): 0.055*\"i\" + 0.039*\"and\" + 0.037*\"the\" + 0.034*\"a\" + 0.026*\"to\" + 0.025*\"my\" + 0.017*\"was\" + 0.016*\"of\" + 0.014*\"in\" + 0.012*\"that\"\n",
      "2018-03-01 23:56:22,191 : INFO : topic #28 (0.010): 0.043*\"games\" + 0.037*\"game\" + 0.022*\"a\" + 0.019*\"and\" + 0.018*\"the\" + 0.017*\"play\" + 0.016*\"d\" + 0.014*\"croissant\" + 0.014*\"sports\" + 0.013*\"try!\"\n",
      "2018-03-01 23:56:22,203 : INFO : topic #29 (0.010): 0.080*\"was\" + 0.078*\"the\" + 0.050*\"and\" + 0.036*\"i\" + 0.033*\"a\" + 0.029*\"it\" + 0.022*\"had\" + 0.015*\"but\" + 0.015*\"very\" + 0.015*\"with\"\n",
      "2018-03-01 23:56:22,203 : INFO : topic #53 (0.010): 0.070*\"the\" + 0.044*\"of\" + 0.031*\"a\" + 0.029*\"and\" + 0.026*\"to\" + 0.023*\"in\" + 0.016*\"that\" + 0.014*\"i\" + 0.014*\"is\" + 0.012*\"you\"\n",
      "2018-03-01 23:56:22,232 : INFO : topic diff=inf, rho=0.229416\n",
      "2018-03-01 23:56:31,384 : INFO : -10.369 per-word bound, 1322.5 perplexity estimate based on a held-out corpus of 2000 documents with 197124 words\n",
      "2018-03-01 23:56:31,384 : INFO : PROGRESS: pass 0, at document #40000/100000\n",
      "2018-03-01 23:56:37,606 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:56:38,851 : INFO : topic #24 (0.010): 0.053*\"the\" + 0.032*\"and\" + 0.023*\"a\" + 0.020*\"sweet\" + 0.017*\"is\" + 0.016*\"i\" + 0.016*\"flavors\" + 0.015*\"but\" + 0.013*\"waffle\" + 0.013*\"their\"\n",
      "2018-03-01 23:56:38,851 : INFO : topic #85 (0.010): 0.035*\"authentic\" + 0.034*\"the\" + 0.029*\"mom\" + 0.024*\"rental\" + 0.020*\"girls\" + 0.018*\"candy\" + 0.018*\"clubs\" + 0.017*\"and\" + 0.017*\"dollar\" + 0.016*\"charging\"\n",
      "2018-03-01 23:56:38,851 : INFO : topic #16 (0.010): 0.061*\"chinese\" + 0.033*\"the\" + 0.024*\"and\" + 0.022*\"sandwiches\" + 0.019*\"owned\" + 0.018*\"is\" + 0.017*\"english\" + 0.016*\"food\" + 0.015*\"original\" + 0.013*\"express\"\n",
      "2018-03-01 23:56:38,851 : INFO : topic #37 (0.010): 0.068*\"the\" + 0.050*\"is\" + 0.031*\"a\" + 0.023*\"but\" + 0.023*\"good\" + 0.020*\"food\" + 0.019*\"and\" + 0.018*\"are\" + 0.015*\"it\" + 0.015*\"to\"\n",
      "2018-03-01 23:56:38,867 : INFO : topic #72 (0.010): 0.069*\"wow!\" + 0.056*\"learn\" + 0.047*\"importantly\" + 0.046*\"respond\" + 0.039*\"cleanliness\" + 0.032*\"ipa\" + 0.025*\"doughy\" + 0.024*\"2013\" + 0.023*\"children's\" + 0.020*\"lee's\"\n",
      "2018-03-01 23:56:38,885 : INFO : topic diff=inf, rho=0.223607\n",
      "2018-03-01 23:56:38,901 : INFO : PROGRESS: pass 0, at document #42000/100000\n",
      "2018-03-01 23:56:45,355 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:56:46,583 : INFO : topic #56 (0.010): 0.053*\"i\" + 0.038*\"and\" + 0.037*\"the\" + 0.034*\"a\" + 0.025*\"to\" + 0.024*\"my\" + 0.017*\"was\" + 0.016*\"of\" + 0.014*\"in\" + 0.011*\"that\"\n",
      "2018-03-01 23:56:46,583 : INFO : topic #70 (0.010): 0.149*\"sauces\" + 0.082*\"concept\" + 0.081*\"everywhere\" + 0.063*\"scene\" + 0.051*\"stupid\" + 0.046*\"pig\" + 0.043*\"for!\" + 0.040*\"puts\" + 0.035*\"neat\" + 0.032*\"whoever\"\n",
      "2018-03-01 23:56:46,583 : INFO : topic #45 (0.010): 0.336*\"brunch\" + 0.147*\"!!\" + 0.057*\"instructors\" + 0.035*\"stephanie\" + 0.015*\"zumba\" + 0.010*\"beth\" + 0.008*\"lasagne\" + 0.007*\"expresso\" + 0.006*\"coupe\" + 0.004*\"bons\"\n",
      "2018-03-01 23:56:46,595 : INFO : topic #9 (0.010): 0.042*\"the\" + 0.034*\"a\" + 0.032*\"and\" + 0.032*\"chocolate\" + 0.017*\"in\" + 0.015*\"der\" + 0.013*\"to\" + 0.012*\"is\" + 0.011*\"for\" + 0.011*\"of\"\n",
      "2018-03-01 23:56:46,595 : INFO : topic #99 (0.010): 0.125*\"maintenance\" + 0.111*\"address\" + 0.081*\"monthly\" + 0.053*\"carte\" + 0.049*\"styles\" + 0.038*\"1000\" + 0.033*\"la\" + 0.026*\"personnel\" + 0.022*\"huevos\" + 0.020*\"bath\"\n",
      "2018-03-01 23:56:46,627 : INFO : topic diff=inf, rho=0.218218\n",
      "2018-03-01 23:56:46,627 : INFO : PROGRESS: pass 0, at document #44000/100000\n",
      "2018-03-01 23:56:53,158 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:56:54,367 : INFO : topic #21 (0.010): 0.160*\"best\" + 0.111*\"ever\" + 0.052*\"i've\" + 0.041*\"delivery\" + 0.032*\"owners\" + 0.029*\"the\" + 0.028*\"house\" + 0.027*\"and\" + 0.024*\"baked\" + 0.019*\"time!\"\n",
      "2018-03-01 23:56:54,367 : INFO : topic #19 (0.010): 0.200*\"roll\" + 0.173*\"rolls\" + 0.159*\"salmon\" + 0.066*\"tuna\" + 0.042*\"spicy\" + 0.034*\"tempura\" + 0.026*\"avocado\" + 0.024*\"fresh\" + 0.016*\"23\" + 0.014*\"rock\"\n",
      "2018-03-01 23:56:54,367 : INFO : topic #40 (0.010): 0.144*\"montreal\" + 0.115*\"favourite\" + 0.075*\"risotto\" + 0.061*\"lo\" + 0.044*\"cirque\" + 0.038*\"unexpected\" + 0.038*\"remind\" + 0.034*\"du\" + 0.031*\"boutique\" + 0.022*\"soleil\"\n",
      "2018-03-01 23:56:54,373 : INFO : topic #38 (0.010): 0.044*\"a\" + 0.038*\"the\" + 0.031*\"and\" + 0.024*\"was\" + 0.021*\"of\" + 0.020*\"i\" + 0.020*\"were\" + 0.014*\"to\" + 0.012*\"with\" + 0.012*\"for\"\n",
      "2018-03-01 23:56:54,373 : INFO : topic #66 (0.010): 0.102*\"fish\" + 0.057*\"happy\" + 0.049*\"great!\" + 0.041*\"hour\" + 0.035*\"tonight\" + 0.031*\"the\" + 0.031*\"pizzas\" + 0.028*\"good!\" + 0.028*\"and\" + 0.023*\"hubby\"\n",
      "2018-03-01 23:56:54,402 : INFO : topic diff=inf, rho=0.213201\n",
      "2018-03-01 23:56:54,402 : INFO : PROGRESS: pass 0, at document #46000/100000\n",
      "2018-03-01 23:57:01,589 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:02,790 : INFO : topic #66 (0.010): 0.100*\"fish\" + 0.063*\"happy\" + 0.047*\"hour\" + 0.044*\"great!\" + 0.037*\"tonight\" + 0.033*\"good!\" + 0.030*\"pizzas\" + 0.030*\"the\" + 0.026*\"and\" + 0.025*\"hubby\"\n",
      "2018-03-01 23:57:02,790 : INFO : topic #37 (0.010): 0.070*\"the\" + 0.049*\"is\" + 0.032*\"a\" + 0.024*\"but\" + 0.023*\"good\" + 0.022*\"food\" + 0.019*\"and\" + 0.018*\"are\" + 0.014*\"to\" + 0.014*\"it\"\n",
      "2018-03-01 23:57:02,790 : INFO : topic #73 (0.010): 0.071*\"1\" + 0.067*\"2\" + 0.057*\"class\" + 0.039*\"here!\" + 0.034*\"3\" + 0.034*\"lol\" + 0.026*\"classes\" + 0.018*\"deli\" + 0.017*\"month\" + 0.016*\"timely\"\n",
      "2018-03-01 23:57:02,795 : INFO : topic #98 (0.010): 0.146*\"photos\" + 0.054*\"nyc\" + 0.037*\"ratio\" + 0.027*\"spotless\" + 0.027*\"hh\" + 0.026*\"fremont\" + 0.023*\"mandarin\" + 0.022*\"cosmo\" + 0.018*\"sin\" + 0.017*\"wasabi\"\n",
      "2018-03-01 23:57:02,795 : INFO : topic #40 (0.010): 0.146*\"montreal\" + 0.130*\"favourite\" + 0.107*\"risotto\" + 0.049*\"lo\" + 0.041*\"cirque\" + 0.037*\"unexpected\" + 0.037*\"remind\" + 0.032*\"du\" + 0.029*\"chinatown\" + 0.027*\"boutique\"\n",
      "2018-03-01 23:57:02,825 : INFO : topic diff=inf, rho=0.208514\n",
      "2018-03-01 23:57:02,825 : INFO : PROGRESS: pass 0, at document #48000/100000\n",
      "2018-03-01 23:57:09,233 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:10,421 : INFO : topic #50 (0.010): 0.114*\"rush\" + 0.085*\"cons\" + 0.077*\"pros\" + 0.053*\"penny\" + 0.050*\"repeat\" + 0.047*\"style\" + 0.042*\"boss\" + 0.039*\"animal\" + 0.034*\"premium\" + 0.029*\"bones\"\n",
      "2018-03-01 23:57:10,421 : INFO : topic #39 (0.010): 0.251*\"amazing\" + 0.151*\"show\" + 0.045*\"app\" + 0.031*\"best!\" + 0.029*\"yesterday\" + 0.025*\"hat\" + 0.024*\"eastern\" + 0.018*\"x\" + 0.018*\"middle\" + 0.014*\"mobile\"\n",
      "2018-03-01 23:57:10,437 : INFO : topic #76 (0.010): 0.109*\"i\" + 0.032*\"to\" + 0.030*\"the\" + 0.029*\"it\" + 0.021*\"that\" + 0.020*\"a\" + 0.019*\"and\" + 0.019*\"but\" + 0.017*\"of\" + 0.016*\"so\"\n",
      "2018-03-01 23:57:10,438 : INFO : topic #29 (0.010): 0.083*\"was\" + 0.079*\"the\" + 0.053*\"and\" + 0.035*\"i\" + 0.034*\"a\" + 0.030*\"it\" + 0.021*\"had\" + 0.015*\"with\" + 0.015*\"but\" + 0.015*\"were\"\n",
      "2018-03-01 23:57:10,438 : INFO : topic #89 (0.010): 0.133*\"wings\" + 0.061*\"back!\" + 0.054*\"groupon\" + 0.030*\"the\" + 0.023*\"consistent\" + 0.021*\"garlic\" + 0.021*\"and\" + 0.016*\"gf\" + 0.014*\"vehicles\" + 0.013*\"a\"\n",
      "2018-03-01 23:57:10,460 : INFO : topic diff=inf, rho=0.204124\n",
      "2018-03-01 23:57:10,476 : INFO : PROGRESS: pass 0, at document #50000/100000\n",
      "2018-03-01 23:57:17,027 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:18,214 : INFO : topic #87 (0.010): 0.025*\"wing\" + 0.024*\"diet\" + 0.023*\"coke\" + 0.022*\"kim\" + 0.020*\"yonge\" + 0.019*\"mmm\" + 0.016*\"chai\" + 0.015*\"a\" + 0.014*\"confirmation\" + 0.014*\"protein\"\n",
      "2018-03-01 23:57:18,214 : INFO : topic #91 (0.010): 0.390*\"cream\" + 0.339*\"ice\" + 0.053*\"coupon\" + 0.027*\"scoop\" + 0.018*\"code\" + 0.015*\"pizza!\" + 0.005*\"hype\" + 0.004*\"humus\" + 0.003*\"i\" + 0.002*\"really?!\"\n",
      "2018-03-01 23:57:18,214 : INFO : topic #78 (0.010): 0.100*\"surgery\" + 0.074*\"grooming\" + 0.042*\"mon\" + 0.037*\"ma\" + 0.035*\"hier\" + 0.034*\"s\" + 0.018*\"grande\" + 0.015*\"se\" + 0.014*\"mal\" + 0.013*\"premier\"\n",
      "2018-03-01 23:57:18,228 : INFO : topic #16 (0.010): 0.068*\"chinese\" + 0.027*\"the\" + 0.025*\"sandwiches\" + 0.024*\"english\" + 0.022*\"owned\" + 0.021*\"and\" + 0.020*\"original\" + 0.018*\"mile\" + 0.016*\"is\" + 0.016*\"bomb\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:57:18,228 : INFO : topic #10 (0.010): 0.329*\"car\" + 0.086*\"wash\" + 0.031*\"computer\" + 0.020*\"con\" + 0.020*\"washed\" + 0.018*\"detail\" + 0.015*\"dropping\" + 0.015*\"jason\" + 0.014*\"clinic\" + 0.010*\"people!\"\n",
      "2018-03-01 23:57:18,249 : INFO : topic diff=inf, rho=0.200000\n",
      "2018-03-01 23:57:18,267 : INFO : PROGRESS: pass 0, at document #52000/100000\n",
      "2018-03-01 23:57:24,676 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:25,932 : INFO : topic #0 (0.010): 0.096*\"the\" + 0.038*\"and\" + 0.029*\"of\" + 0.028*\"a\" + 0.026*\"with\" + 0.018*\"was\" + 0.017*\"salad\" + 0.015*\"steak\" + 0.013*\"i\" + 0.012*\"for\"\n",
      "2018-03-01 23:57:25,932 : INFO : topic #11 (0.010): 0.391*\"club\" + 0.094*\"grand\" + 0.054*\"switched\" + 0.034*\"wipe\" + 0.029*\"comedy\" + 0.024*\"hangover\" + 0.022*\"ears\" + 0.021*\"manger\" + 0.015*\"angela\" + 0.011*\"hairdressers\"\n",
      "2018-03-01 23:57:25,937 : INFO : topic #57 (0.010): 0.128*\"turkey\" + 0.090*\"country\" + 0.051*\"jack\" + 0.050*\"boxes\" + 0.040*\"brownie\" + 0.037*\"alfredo\" + 0.033*\"wonderful!\" + 0.030*\"adorable\" + 0.026*\"competitive\" + 0.023*\"box\"\n",
      "2018-03-01 23:57:25,937 : INFO : topic #94 (0.010): 0.204*\"street\" + 0.136*\"across\" + 0.093*\"outstanding\" + 0.070*\"accommodating\" + 0.069*\"pancakes\" + 0.067*\"knowledgeable\" + 0.055*\"mood\" + 0.030*\"garden\" + 0.026*\"rose\" + 0.020*\"love\"\n",
      "2018-03-01 23:57:25,937 : INFO : topic #40 (0.010): 0.164*\"favourite\" + 0.120*\"montreal\" + 0.085*\"risotto\" + 0.051*\"lo\" + 0.048*\"unexpected\" + 0.038*\"remind\" + 0.035*\"cirque\" + 0.034*\"boutique\" + 0.027*\"chinatown\" + 0.025*\"corners\"\n",
      "2018-03-01 23:57:25,972 : INFO : topic diff=inf, rho=0.196116\n",
      "2018-03-01 23:57:25,972 : INFO : PROGRESS: pass 0, at document #54000/100000\n",
      "2018-03-01 23:57:32,360 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:33,602 : INFO : topic #77 (0.010): 0.052*\"i\" + 0.044*\"a\" + 0.042*\"and\" + 0.037*\"to\" + 0.032*\"was\" + 0.030*\"my\" + 0.025*\"the\" + 0.020*\"very\" + 0.018*\"for\" + 0.014*\"ago\"\n",
      "2018-03-01 23:57:33,602 : INFO : topic #42 (0.010): 0.061*\"the\" + 0.056*\"a\" + 0.037*\"and\" + 0.031*\"of\" + 0.023*\"to\" + 0.022*\"was\" + 0.016*\"for\" + 0.014*\"with\" + 0.014*\"in\" + 0.012*\"on\"\n",
      "2018-03-01 23:57:33,602 : INFO : topic #83 (0.010): 0.074*\"the\" + 0.037*\"to\" + 0.029*\"a\" + 0.025*\"and\" + 0.023*\"of\" + 0.020*\"was\" + 0.018*\"i\" + 0.016*\"in\" + 0.014*\"that\" + 0.014*\"it\"\n",
      "2018-03-01 23:57:33,608 : INFO : topic #38 (0.010): 0.040*\"a\" + 0.034*\"the\" + 0.029*\"and\" + 0.021*\"event\" + 0.021*\"was\" + 0.019*\"were\" + 0.019*\"of\" + 0.016*\"i\" + 0.014*\"enjoyable\" + 0.013*\"to\"\n",
      "2018-03-01 23:57:33,608 : INFO : topic #61 (0.010): 0.076*\"card\" + 0.044*\"credit\" + 0.033*\"cash\" + 0.029*\"they\" + 0.021*\"and\" + 0.019*\"you!\" + 0.019*\"vet\" + 0.018*\"cards\" + 0.018*\"to\" + 0.016*\"the\"\n",
      "2018-03-01 23:57:33,643 : INFO : topic diff=inf, rho=0.192450\n",
      "2018-03-01 23:57:33,643 : INFO : PROGRESS: pass 0, at document #56000/100000\n",
      "2018-03-01 23:57:40,598 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:41,791 : INFO : topic #3 (0.010): 0.130*\"dont\" + 0.081*\"bucks\" + 0.080*\"didnt\" + 0.068*\"san\" + 0.032*\"million\" + 0.031*\"wasnt\" + 0.024*\"fear\" + 0.017*\"quinoa\" + 0.016*\"done!\" + 0.016*\"tropical\"\n",
      "2018-03-01 23:57:41,806 : INFO : topic #22 (0.010): 0.066*\"the\" + 0.044*\"and\" + 0.034*\"a\" + 0.027*\"i\" + 0.017*\"of\" + 0.017*\"with\" + 0.016*\"sandwich\" + 0.015*\"to\" + 0.013*\"was\" + 0.013*\"it\"\n",
      "2018-03-01 23:57:41,809 : INFO : topic #80 (0.010): 0.074*\"you\" + 0.043*\"to\" + 0.032*\"the\" + 0.028*\"and\" + 0.028*\"they\" + 0.024*\"if\" + 0.022*\"your\" + 0.022*\"is\" + 0.020*\"a\" + 0.018*\"are\"\n",
      "2018-03-01 23:57:41,809 : INFO : topic #14 (0.010): 0.150*\"coffee\" + 0.038*\"a\" + 0.029*\"cup\" + 0.022*\"the\" + 0.020*\"shop\" + 0.018*\"starbucks\" + 0.017*\"tea\" + 0.016*\"espresso\" + 0.016*\"i\" + 0.015*\"of\"\n",
      "2018-03-01 23:57:41,809 : INFO : topic #42 (0.010): 0.061*\"the\" + 0.056*\"a\" + 0.037*\"and\" + 0.031*\"of\" + 0.023*\"to\" + 0.022*\"was\" + 0.016*\"for\" + 0.014*\"with\" + 0.014*\"in\" + 0.012*\"on\"\n",
      "2018-03-01 23:57:41,832 : INFO : topic diff=inf, rho=0.188982\n",
      "2018-03-01 23:57:41,848 : INFO : PROGRESS: pass 0, at document #58000/100000\n",
      "2018-03-01 23:57:48,321 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:57:49,517 : INFO : topic #0 (0.010): 0.095*\"the\" + 0.039*\"and\" + 0.029*\"of\" + 0.028*\"a\" + 0.026*\"with\" + 0.017*\"was\" + 0.016*\"steak\" + 0.016*\"salad\" + 0.012*\"i\" + 0.012*\"for\"\n",
      "2018-03-01 23:57:49,517 : INFO : topic #13 (0.010): 0.087*\"buffet\" + 0.066*\"priced\" + 0.047*\"food\" + 0.040*\"pasta\" + 0.037*\"italian\" + 0.035*\"indian\" + 0.029*\"dishes\" + 0.028*\"reasonably\" + 0.027*\"seafood\" + 0.027*\"and\"\n",
      "2018-03-01 23:57:49,517 : INFO : topic #29 (0.010): 0.085*\"was\" + 0.083*\"the\" + 0.055*\"and\" + 0.035*\"i\" + 0.034*\"a\" + 0.030*\"it\" + 0.022*\"had\" + 0.016*\"were\" + 0.015*\"with\" + 0.014*\"very\"\n",
      "2018-03-01 23:57:49,531 : INFO : topic #99 (0.010): 0.117*\"maintenance\" + 0.108*\"address\" + 0.091*\"monthly\" + 0.056*\"carte\" + 0.054*\"styles\" + 0.038*\"la\" + 0.033*\"personnel\" + 0.031*\"1000\" + 0.028*\"mazda\" + 0.024*\"bath\"\n",
      "2018-03-01 23:57:49,531 : INFO : topic #11 (0.010): 0.377*\"club\" + 0.119*\"grand\" + 0.041*\"switched\" + 0.033*\"hangover\" + 0.032*\"wipe\" + 0.026*\"comedy\" + 0.023*\"ears\" + 0.022*\"angela\" + 0.022*\"manger\" + 0.018*\"dye\"\n",
      "2018-03-01 23:57:49,565 : INFO : topic diff=inf, rho=0.185695\n",
      "2018-03-01 23:57:58,986 : INFO : -10.376 per-word bound, 1328.5 perplexity estimate based on a held-out corpus of 2000 documents with 210596 words\n",
      "2018-03-01 23:57:58,986 : INFO : PROGRESS: pass 0, at document #60000/100000\n",
      "2018-03-01 23:58:05,296 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:06,471 : INFO : topic #1 (0.010): 0.069*\"and\" + 0.043*\"he\" + 0.038*\"my\" + 0.033*\"to\" + 0.029*\"i\" + 0.024*\"his\" + 0.021*\"with\" + 0.019*\"for\" + 0.015*\"me\" + 0.015*\"the\"\n",
      "2018-03-01 23:58:06,471 : INFO : topic #71 (0.010): 0.342*\"sushi\" + 0.176*\"drive\" + 0.056*\"thru\" + 0.043*\"chef\" + 0.018*\"friendly!\" + 0.017*\"nigiri\" + 0.016*\"menu!\" + 0.012*\"us!\" + 0.011*\"service\" + 0.010*\"employees\"\n",
      "2018-03-01 23:58:06,471 : INFO : topic #84 (0.010): 0.074*\"art\" + 0.068*\"sub\" + 0.061*\"basic\" + 0.052*\"bartenders\" + 0.050*\"par\" + 0.028*\"law\" + 0.025*\"canadian\" + 0.025*\"list\" + 0.023*\"collection\" + 0.021*\"canada\"\n",
      "2018-03-01 23:58:06,471 : INFO : topic #19 (0.010): 0.188*\"roll\" + 0.186*\"rolls\" + 0.150*\"salmon\" + 0.094*\"tuna\" + 0.047*\"spicy\" + 0.037*\"tempura\" + 0.028*\"fresh\" + 0.027*\"avocado\" + 0.015*\"23\" + 0.013*\"sushi\"\n",
      "2018-03-01 23:58:06,484 : INFO : topic #82 (0.010): 0.105*\"tip\" + 0.082*\"vehicle\" + 0.079*\"slice\" + 0.070*\"charlotte\" + 0.062*\"range\" + 0.052*\"driving\" + 0.049*\"bigger\" + 0.033*\"changing\" + 0.030*\"central\" + 0.028*\"0\"\n",
      "2018-03-01 23:58:06,509 : INFO : topic diff=inf, rho=0.182574\n",
      "2018-03-01 23:58:06,525 : INFO : PROGRESS: pass 0, at document #62000/100000\n",
      "2018-03-01 23:58:12,741 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:13,932 : INFO : topic #40 (0.010): 0.146*\"favourite\" + 0.113*\"montreal\" + 0.088*\"risotto\" + 0.051*\"cirque\" + 0.049*\"lo\" + 0.041*\"boutique\" + 0.037*\"remind\" + 0.032*\"unexpected\" + 0.030*\"du\" + 0.027*\"corners\"\n",
      "2018-03-01 23:58:13,932 : INFO : topic #70 (0.010): 0.143*\"sauces\" + 0.089*\"everywhere\" + 0.082*\"concept\" + 0.066*\"h\" + 0.051*\"neat\" + 0.049*\"scene\" + 0.046*\"stupid\" + 0.038*\"pig\" + 0.036*\"for!\" + 0.028*\"puts\"\n",
      "2018-03-01 23:58:13,932 : INFO : topic #14 (0.010): 0.163*\"coffee\" + 0.038*\"a\" + 0.029*\"cup\" + 0.027*\"shop\" + 0.020*\"the\" + 0.020*\"starbucks\" + 0.018*\"tea\" + 0.016*\"latte\" + 0.014*\"i\" + 0.014*\"of\"\n",
      "2018-03-01 23:58:13,942 : INFO : topic #34 (0.010): 0.122*\"we\" + 0.052*\"and\" + 0.049*\"our\" + 0.047*\"the\" + 0.033*\"to\" + 0.031*\"were\" + 0.026*\"was\" + 0.026*\"us\" + 0.019*\"a\" + 0.014*\"had\"\n",
      "2018-03-01 23:58:13,942 : INFO : topic #53 (0.010): 0.076*\"the\" + 0.051*\"of\" + 0.036*\"a\" + 0.031*\"and\" + 0.027*\"to\" + 0.024*\"in\" + 0.017*\"that\" + 0.015*\"is\" + 0.013*\"with\" + 0.011*\"on\"\n",
      "2018-03-01 23:58:13,963 : INFO : topic diff=inf, rho=0.179605\n",
      "2018-03-01 23:58:13,978 : INFO : PROGRESS: pass 0, at document #64000/100000\n",
      "2018-03-01 23:58:20,423 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:21,598 : INFO : topic #42 (0.010): 0.062*\"the\" + 0.057*\"a\" + 0.036*\"and\" + 0.031*\"of\" + 0.023*\"to\" + 0.022*\"was\" + 0.017*\"for\" + 0.014*\"in\" + 0.014*\"with\" + 0.012*\"bar\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:58:21,598 : INFO : topic #15 (0.010): 0.034*\"az\" + 0.029*\"mark\" + 0.026*\"tom\" + 0.022*\"the\" + 0.021*\"ac\" + 0.021*\"worker\" + 0.020*\"hill\" + 0.020*\"budget\" + 0.017*\"rate\" + 0.016*\"missed\"\n",
      "2018-03-01 23:58:21,598 : INFO : topic #86 (0.010): 0.042*\"a\" + 0.034*\"and\" + 0.026*\"the\" + 0.025*\"of\" + 0.021*\"wife\" + 0.016*\"they\" + 0.016*\"to\" + 0.015*\"is\" + 0.013*\"for\" + 0.013*\"in\"\n",
      "2018-03-01 23:58:21,612 : INFO : topic #10 (0.010): 0.376*\"car\" + 0.077*\"wash\" + 0.028*\"computer\" + 0.023*\"clinic\" + 0.021*\"washed\" + 0.020*\"con\" + 0.016*\"jason\" + 0.015*\"people!\" + 0.014*\"dropping\" + 0.014*\"detail\"\n",
      "2018-03-01 23:58:21,612 : INFO : topic #31 (0.010): 0.289*\"french\" + 0.193*\"toast\" + 0.098*\"yum\" + 0.043*\"soooo\" + 0.043*\"satisfy\" + 0.034*\"shirts\" + 0.027*\"denny's\" + 0.018*\"spoon\" + 0.016*\"beats\" + 0.014*\"closet\"\n",
      "2018-03-01 23:58:21,634 : INFO : topic diff=inf, rho=0.176777\n",
      "2018-03-01 23:58:21,650 : INFO : PROGRESS: pass 0, at document #66000/100000\n",
      "2018-03-01 23:58:27,981 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:29,171 : INFO : topic #0 (0.010): 0.095*\"the\" + 0.039*\"and\" + 0.029*\"of\" + 0.028*\"with\" + 0.028*\"a\" + 0.017*\"salad\" + 0.017*\"steak\" + 0.016*\"was\" + 0.012*\"for\" + 0.012*\"i\"\n",
      "2018-03-01 23:58:29,171 : INFO : topic #10 (0.010): 0.381*\"car\" + 0.080*\"wash\" + 0.028*\"computer\" + 0.025*\"clinic\" + 0.020*\"washed\" + 0.020*\"con\" + 0.017*\"detail\" + 0.015*\"dropping\" + 0.013*\"jason\" + 0.013*\"people!\"\n",
      "2018-03-01 23:58:29,171 : INFO : topic #59 (0.010): 0.166*\"closed\" + 0.111*\"tons\" + 0.069*\"changes\" + 0.050*\"businesses\" + 0.041*\"hawaiian\" + 0.038*\"local\" + 0.033*\"enchilada\" + 0.031*\"chop\" + 0.029*\"closest\" + 0.024*\"5pm\"\n",
      "2018-03-01 23:58:29,178 : INFO : topic #87 (0.010): 0.032*\"wing\" + 0.028*\"coke\" + 0.024*\"diet\" + 0.018*\"alot\" + 0.018*\"kim\" + 0.018*\"chai\" + 0.017*\"mmm\" + 0.016*\"holy\" + 0.016*\"yonge\" + 0.015*\"bloor\"\n",
      "2018-03-01 23:58:29,178 : INFO : topic #18 (0.010): 0.062*\"the\" + 0.049*\"was\" + 0.040*\"i\" + 0.030*\"to\" + 0.030*\"it\" + 0.029*\"and\" + 0.018*\"food\" + 0.017*\"not\" + 0.014*\"a\" + 0.014*\"but\"\n",
      "2018-03-01 23:58:29,207 : INFO : topic diff=inf, rho=0.174078\n",
      "2018-03-01 23:58:29,207 : INFO : PROGRESS: pass 0, at document #68000/100000\n",
      "2018-03-01 23:58:35,642 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:36,829 : INFO : topic #0 (0.010): 0.095*\"the\" + 0.039*\"and\" + 0.029*\"of\" + 0.028*\"with\" + 0.028*\"a\" + 0.018*\"salad\" + 0.016*\"steak\" + 0.016*\"was\" + 0.012*\"for\" + 0.012*\"i\"\n",
      "2018-03-01 23:58:36,829 : INFO : topic #76 (0.010): 0.111*\"i\" + 0.034*\"to\" + 0.030*\"the\" + 0.030*\"it\" + 0.022*\"that\" + 0.021*\"a\" + 0.019*\"but\" + 0.019*\"and\" + 0.016*\"so\" + 0.016*\"of\"\n",
      "2018-03-01 23:58:36,843 : INFO : topic #12 (0.010): 0.210*\"pizza\" + 0.038*\"the\" + 0.032*\"crust\" + 0.032*\"and\" + 0.030*\"a\" + 0.026*\"cheese\" + 0.020*\"pie\" + 0.018*\"thin\" + 0.018*\"toppings\" + 0.018*\"sauce\"\n",
      "2018-03-01 23:58:36,843 : INFO : topic #47 (0.010): 0.070*\"the\" + 0.035*\"a\" + 0.032*\"to\" + 0.027*\"and\" + 0.022*\"of\" + 0.017*\"is\" + 0.017*\"in\" + 0.013*\"that\" + 0.012*\"it\" + 0.012*\"for\"\n",
      "2018-03-01 23:58:36,847 : INFO : topic #74 (0.010): 0.065*\"the\" + 0.048*\"and\" + 0.033*\"of\" + 0.030*\"a\" + 0.023*\"is\" + 0.017*\"with\" + 0.016*\"chicken\" + 0.013*\"for\" + 0.011*\"in\" + 0.010*\"meat\"\n",
      "2018-03-01 23:58:36,865 : INFO : topic diff=inf, rho=0.171499\n",
      "2018-03-01 23:58:36,881 : INFO : PROGRESS: pass 0, at document #70000/100000\n",
      "2018-03-01 23:58:43,507 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:44,697 : INFO : topic #10 (0.010): 0.392*\"car\" + 0.082*\"wash\" + 0.029*\"computer\" + 0.024*\"clinic\" + 0.021*\"con\" + 0.020*\"washed\" + 0.016*\"dropping\" + 0.014*\"jason\" + 0.013*\"site\" + 0.012*\"detail\"\n",
      "2018-03-01 23:58:44,697 : INFO : topic #46 (0.010): 0.105*\"and\" + 0.063*\"the\" + 0.056*\"very\" + 0.056*\"is\" + 0.044*\"great\" + 0.041*\"friendly\" + 0.039*\"staff\" + 0.037*\"service\" + 0.029*\"are\" + 0.025*\"always\"\n",
      "2018-03-01 23:58:44,697 : INFO : topic #55 (0.010): 0.382*\"recommend\" + 0.213*\"highly\" + 0.050*\"would\" + 0.038*\"great\" + 0.018*\"chicken\" + 0.015*\"build\" + 0.012*\"and\" + 0.012*\"design\" + 0.010*\"helpful\" + 0.008*\"restaurant!\"\n",
      "2018-03-01 23:58:44,705 : INFO : topic #13 (0.010): 0.088*\"buffet\" + 0.070*\"priced\" + 0.057*\"pasta\" + 0.048*\"food\" + 0.043*\"italian\" + 0.035*\"indian\" + 0.032*\"dishes\" + 0.030*\"reasonably\" + 0.027*\"seafood\" + 0.024*\"for\"\n",
      "2018-03-01 23:58:44,705 : INFO : topic #43 (0.010): 0.054*\"the\" + 0.026*\"and\" + 0.024*\"of\" + 0.020*\"tea\" + 0.020*\"i\" + 0.018*\"with\" + 0.018*\"it\" + 0.017*\"pork\" + 0.017*\"a\" + 0.015*\"to\"\n",
      "2018-03-01 23:58:44,724 : INFO : topic diff=inf, rho=0.169031\n",
      "2018-03-01 23:58:44,740 : INFO : PROGRESS: pass 0, at document #72000/100000\n",
      "2018-03-01 23:58:50,942 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:52,111 : INFO : topic #34 (0.010): 0.123*\"we\" + 0.051*\"and\" + 0.050*\"our\" + 0.047*\"the\" + 0.034*\"to\" + 0.032*\"were\" + 0.030*\"us\" + 0.025*\"was\" + 0.018*\"a\" + 0.015*\"had\"\n",
      "2018-03-01 23:58:52,111 : INFO : topic #66 (0.010): 0.146*\"fish\" + 0.093*\"happy\" + 0.078*\"hour\" + 0.064*\"great!\" + 0.044*\"tonight\" + 0.043*\"pizzas\" + 0.034*\"hubby\" + 0.032*\"good!\" + 0.021*\"yummy\" + 0.019*\"the\"\n",
      "2018-03-01 23:58:52,127 : INFO : topic #87 (0.010): 0.029*\"coke\" + 0.028*\"wing\" + 0.028*\"diet\" + 0.019*\"chai\" + 0.018*\"powder\" + 0.017*\"punch\" + 0.016*\"holy\" + 0.015*\"kim\" + 0.014*\"rum\" + 0.014*\"conversations\"\n",
      "2018-03-01 23:58:52,127 : INFO : topic #72 (0.010): 0.093*\"wow!\" + 0.083*\"learn\" + 0.047*\"cleanliness\" + 0.046*\"importantly\" + 0.037*\"respond\" + 0.033*\"royal\" + 0.031*\"2013\" + 0.029*\"extreme\" + 0.020*\"treatments\" + 0.020*\"ipa\"\n",
      "2018-03-01 23:58:52,127 : INFO : topic #27 (0.010): 0.099*\"exceptional\" + 0.055*\"paint\" + 0.053*\"custom\" + 0.039*\"created\" + 0.036*\"cheapest\" + 0.031*\"pretzel\" + 0.029*\"project\" + 0.025*\"google\" + 0.023*\"right!\" + 0.020*\"features\"\n",
      "2018-03-01 23:58:52,152 : INFO : topic diff=inf, rho=0.166667\n",
      "2018-03-01 23:58:52,168 : INFO : PROGRESS: pass 0, at document #74000/100000\n",
      "2018-03-01 23:58:58,604 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:58:59,798 : INFO : topic #12 (0.010): 0.211*\"pizza\" + 0.037*\"the\" + 0.033*\"crust\" + 0.030*\"and\" + 0.030*\"cheese\" + 0.029*\"a\" + 0.019*\"thin\" + 0.019*\"toppings\" + 0.018*\"sauce\" + 0.017*\"pie\"\n",
      "2018-03-01 23:58:59,798 : INFO : topic #37 (0.010): 0.075*\"the\" + 0.051*\"is\" + 0.033*\"a\" + 0.026*\"but\" + 0.025*\"good\" + 0.023*\"food\" + 0.019*\"are\" + 0.018*\"and\" + 0.015*\"not\" + 0.014*\"it\"\n",
      "2018-03-01 23:58:59,798 : INFO : topic #41 (0.010): 0.213*\"13\" + 0.116*\"!!!\" + 0.101*\"hamburger\" + 0.059*\"ya\" + 0.055*\"monte\" + 0.053*\"frites\" + 0.010*\"chimi\" + 0.009*\"pommes\" + 0.005*\"excellente\" + 0.004*\"canard\"\n",
      "2018-03-01 23:58:59,809 : INFO : topic #61 (0.010): 0.081*\"card\" + 0.052*\"credit\" + 0.044*\"cash\" + 0.024*\"cards\" + 0.024*\"they\" + 0.021*\"you!\" + 0.021*\"vet\" + 0.018*\"cat\" + 0.017*\"register\" + 0.016*\"trash\"\n",
      "2018-03-01 23:58:59,809 : INFO : topic #93 (0.010): 0.027*\"the\" + 0.016*\"to\" + 0.016*\"and\" + 0.015*\"tax\" + 0.015*\"a\" + 0.014*\"i\" + 0.013*\"of\" + 0.011*\"deposit\" + 0.010*\"report\" + 0.009*\"with\"\n",
      "2018-03-01 23:58:59,838 : INFO : topic diff=inf, rho=0.164399\n",
      "2018-03-01 23:58:59,838 : INFO : PROGRESS: pass 0, at document #76000/100000\n",
      "2018-03-01 23:59:06,098 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:59:07,284 : INFO : topic #64 (0.010): 0.039*\"17\" + 0.024*\"hire\" + 0.022*\"pedi\" + 0.021*\"smoothie\" + 0.020*\"farm\" + 0.019*\"martini\" + 0.019*\"more!\" + 0.018*\"leak\" + 0.017*\"experience!\" + 0.016*\"mani\"\n",
      "2018-03-01 23:59:07,284 : INFO : topic #78 (0.010): 0.127*\"s\" + 0.111*\"personal\" + 0.091*\"surgery\" + 0.059*\"grooming\" + 0.052*\"hier\" + 0.037*\"mon\" + 0.026*\"mal\" + 0.019*\"grande\" + 0.016*\"sans\" + 0.015*\"fois\"\n",
      "2018-03-01 23:59:07,284 : INFO : topic #45 (0.010): 0.443*\"brunch\" + 0.145*\"!!\" + 0.042*\"instructors\" + 0.024*\"stephanie\" + 0.011*\"zumba\" + 0.011*\"bons\" + 0.010*\"expresso\" + 0.009*\"pro!\" + 0.005*\"toilette\" + 0.004*\"thé\"\n",
      "2018-03-01 23:59:07,287 : INFO : topic #23 (0.010): 0.047*\"the\" + 0.040*\"to\" + 0.031*\"kids\" + 0.030*\"a\" + 0.022*\"and\" + 0.016*\"for\" + 0.012*\"in\" + 0.012*\"of\" + 0.010*\"they\" + 0.009*\"this\"\n",
      "2018-03-01 23:59:07,287 : INFO : topic #10 (0.010): 0.410*\"car\" + 0.067*\"wash\" + 0.030*\"con\" + 0.030*\"computer\" + 0.024*\"clinic\" + 0.016*\"jason\" + 0.015*\"dropping\" + 0.014*\"people!\" + 0.014*\"washed\" + 0.013*\"site\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 23:59:07,319 : INFO : topic diff=inf, rho=0.162221\n",
      "2018-03-01 23:59:07,324 : INFO : PROGRESS: pass 0, at document #78000/100000\n",
      "2018-03-01 23:59:14,050 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:59:15,267 : INFO : topic #24 (0.010): 0.044*\"the\" + 0.031*\"and\" + 0.027*\"flavors\" + 0.026*\"sweet\" + 0.020*\"a\" + 0.018*\"their\" + 0.017*\"waffle\" + 0.015*\"waffles\" + 0.014*\"delicious\" + 0.013*\"strawberry\"\n",
      "2018-03-01 23:59:15,267 : INFO : topic #44 (0.010): 0.027*\"the\" + 0.027*\"i\" + 0.026*\"a\" + 0.020*\"was\" + 0.019*\"and\" + 0.019*\"office\" + 0.017*\"to\" + 0.015*\"in\" + 0.011*\"apartment\" + 0.010*\"that\"\n",
      "2018-03-01 23:59:15,272 : INFO : topic #60 (0.010): 0.100*\"the\" + 0.036*\"is\" + 0.034*\"in\" + 0.032*\"and\" + 0.031*\"a\" + 0.031*\"vegas\" + 0.025*\"to\" + 0.022*\"of\" + 0.013*\"are\" + 0.013*\"at\"\n",
      "2018-03-01 23:59:15,272 : INFO : topic #49 (0.010): 0.070*\"gin\" + 0.067*\"attraction\" + 0.050*\"su\" + 0.050*\"charcuterie\" + 0.011*\"hangs\" + 0.009*\"nouveau\" + 0.008*\"knowledgeable!\" + 0.008*\"lego\" + 0.005*\"boustan\" + 0.004*\"legos\"\n",
      "2018-03-01 23:59:15,276 : INFO : topic #42 (0.010): 0.062*\"the\" + 0.057*\"a\" + 0.035*\"and\" + 0.033*\"of\" + 0.023*\"to\" + 0.021*\"was\" + 0.017*\"for\" + 0.014*\"in\" + 0.014*\"with\" + 0.014*\"at\"\n",
      "2018-03-01 23:59:15,306 : INFO : topic diff=inf, rho=0.160128\n",
      "2018-03-01 23:59:24,510 : INFO : -10.561 per-word bound, 1510.9 perplexity estimate based on a held-out corpus of 2000 documents with 214479 words\n",
      "2018-03-01 23:59:24,510 : INFO : PROGRESS: pass 0, at document #80000/100000\n",
      "2018-03-01 23:59:30,868 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:59:32,075 : INFO : topic #39 (0.010): 0.302*\"amazing\" + 0.180*\"show\" + 0.042*\"app\" + 0.035*\"yesterday\" + 0.028*\"best!\" + 0.026*\"hat\" + 0.024*\"x\" + 0.018*\"march\" + 0.016*\"fiance\" + 0.015*\"eastern\"\n",
      "2018-03-01 23:59:32,075 : INFO : topic #35 (0.010): 0.063*\"prices\" + 0.047*\"for\" + 0.041*\"reasonable\" + 0.040*\"price\" + 0.031*\"the\" + 0.027*\"store\" + 0.026*\"and\" + 0.024*\"is\" + 0.022*\"a\" + 0.022*\"quality\"\n",
      "2018-03-01 23:59:32,075 : INFO : topic #26 (0.010): 0.041*\"the\" + 0.020*\"a\" + 0.019*\"and\" + 0.014*\"of\" + 0.012*\"balance\" + 0.011*\"i\" + 0.010*\"it\" + 0.008*\"with\" + 0.007*\"in\" + 0.007*\"cone\"\n",
      "2018-03-01 23:59:32,081 : INFO : topic #0 (0.010): 0.092*\"the\" + 0.039*\"and\" + 0.029*\"with\" + 0.029*\"of\" + 0.028*\"a\" + 0.019*\"salad\" + 0.015*\"was\" + 0.015*\"steak\" + 0.012*\"for\" + 0.011*\"to\"\n",
      "2018-03-01 23:59:32,081 : INFO : topic #48 (0.010): 0.651*\"!\" + 0.012*\"brunswick\" + 0.007*\"street!\" + 0.005*\"sa\" + 0.004*\"cuba\" + 0.002*\"incontournable\" + 0.002*\"flicker\" + 0.002*\"munchkins\" + 0.002*\"crass\" + 0.002*\"joue\"\n",
      "2018-03-01 23:59:32,103 : INFO : topic diff=inf, rho=0.158114\n",
      "2018-03-01 23:59:32,119 : INFO : PROGRESS: pass 0, at document #82000/100000\n",
      "2018-03-01 23:59:38,597 : INFO : merging changes from 2000 documents into a model of 100000 documents\n",
      "2018-03-01 23:59:39,802 : INFO : topic #32 (0.010): 0.091*\"the\" + 0.038*\"was\" + 0.037*\"room\" + 0.033*\"and\" + 0.031*\"a\" + 0.022*\"to\" + 0.021*\"in\" + 0.015*\"hotel\" + 0.015*\"for\" + 0.012*\"were\"\n",
      "2018-03-01 23:59:39,817 : INFO : topic #16 (0.010): 0.081*\"chinese\" + 0.037*\"original\" + 0.037*\"owned\" + 0.033*\"sandwiches\" + 0.030*\"english\" + 0.023*\"bomb\" + 0.021*\"amazingly\" + 0.020*\"eats\" + 0.018*\"american\" + 0.018*\"mile\"\n",
      "2018-03-01 23:59:39,820 : INFO : topic #25 (0.010): 0.114*\"pleasure\" + 0.113*\"repairs\" + 0.107*\"knowledgable\" + 0.084*\"gun\" + 0.066*\"he's\" + 0.058*\"branch\" + 0.043*\"lately\" + 0.029*\"freshest\" + 0.025*\"shop!\" + 0.021*\"link\"\n",
      "2018-03-01 23:59:39,820 : INFO : topic #74 (0.010): 0.064*\"the\" + 0.048*\"and\" + 0.034*\"of\" + 0.030*\"a\" + 0.022*\"is\" + 0.018*\"with\" + 0.017*\"chicken\" + 0.013*\"for\" + 0.011*\"in\" + 0.010*\"meat\"\n",
      "2018-03-01 23:59:39,820 : INFO : topic #66 (0.010): 0.134*\"fish\" + 0.123*\"happy\" + 0.113*\"hour\" + 0.067*\"great!\" + 0.045*\"pizzas\" + 0.043*\"tonight\" + 0.036*\"hubby\" + 0.027*\"good!\" + 0.018*\"yummy\" + 0.016*\"the\"\n",
      "2018-03-01 23:59:39,845 : INFO : topic diff=inf, rho=0.156174\n",
      "2018-03-01 23:59:39,864 : INFO : PROGRESS: pass 0, at document #84000/100000\n"
     ]
    }
   ],
   "source": [
    "# print('-------mutual information feature selection-------')\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# dictionary = corpora.Dictionary(x_lower)\n",
    "# corpus = [dictionary.doc2bow(text) for text in x_lower]\n",
    "# model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "\n",
    "\n",
    "# def bow_to_matrix(bow,lda_model):\n",
    "\n",
    "#     result = np.zeros((len(bow),lda_model.get_topics().shape[0]),dtype=np.float64)\n",
    "\n",
    "#     for i,each in enumerate(bow):\n",
    "\n",
    "#         # each format as [(31,1),(38,1),(40,1)]\n",
    "\n",
    "#         l = lda_model.get_document_topics(each) #l format as [(27, 0.20886971), (34, 0.17654318), (41, 0.50100213), (86, 0.060251668)]\n",
    "\n",
    "#         a = [each[0] for each in l]\n",
    "\n",
    "#         b = [each[1] for each in l]\n",
    "\n",
    "#         result[i,a] = b     \n",
    "\n",
    "#     result = csr_matrix(result)\n",
    "\n",
    "#     return result\n",
    "# youbian = bow_to_matrix(corpus,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_matrix = scaler.fit_transform(youbian)\n",
    "# X = hstack([X,lda_matirx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add NOT_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Ridge-------\n",
      "train accuracy: 0.8340149253731344\n",
      "test accuracy: 0.6690909090909091\n",
      "time elapsed:  31.54082465171814\n"
     ]
    }
   ],
   "source": [
    "print('-------Ridge-------')\n",
    "s = time.time()\n",
    "# rkf = RepeatedKFold(n_splits=2, n_repeats=1)\n",
    "# parameters = {'alpha':[5]}\n",
    "# accu_score = make_scorer(accuracy_score)\n",
    "# ridge_model = RidgeClassifier()\n",
    "# ridge_cv = GridSearchCV(ridge_model, parameters,cv=rkf, pre_dispatch=4, return_train_score = True,scoring=accu_score)\n",
    "# ridge_cv.fit(x_train, y_train)\n",
    "# print(ridge_cv.cv_results_)\n",
    "# ridge_train_res = ridge_cv.predict(x_train)\n",
    "# ridge_test_res = ridge_cv.predict(x_test)\n",
    "ridge_model = RidgeClassifier(alpha = 3)\n",
    "ridge_model = ridge_model.fit(x_train, y_train)\n",
    "ridge_train_res = ridge_model.predict(x_train)\n",
    "ridge_test_res = ridge_model.predict(x_test)\n",
    "print(\"train accuracy:\", accuracy_score(ridge_train_res, y_train))\n",
    "print(\"test accuracy:\", accuracy_score(ridge_test_res, y_test))\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "num_train, num_feature = x_train.shape\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, free_raw_data=False)\n",
    "lgb_test = lgb.Dataset(x_test, y_test, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclassova',\n",
    "    'metric': 'multi_error',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_class': 5\n",
    "}\n",
    "print('Start training...')\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_test,\n",
    "               early_stopping_rounds=5,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.645636"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
